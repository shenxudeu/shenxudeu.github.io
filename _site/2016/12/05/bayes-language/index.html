<p>Bayesian Language</p>

<!-- The Normal Distribution -->
<div class="equation" data-expr="\displaystyle P(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma ^2}}"></div>

<hr />
<p>I <script type="math/tex">X_i+ y</script> have studied Bayesian modeling several times since 2010, read a lot paper, couple of books, and some online classes. Yes, you can remember math terms, follow what they said in paper, sometimes even the equation derivations. But I never really understand what’s going on behind those equations. That’s the reason I have to repeat the classes, but stuck in a loop. Every time I learn it, I thought I got the idea, and over and over again.</p>

<p>That’s because there is a Bayesian language and way to think about data modeling, which is different with deterministic modeling such as Neural Networks. We should try to link every concept in Bayesian to Neural Networks. Because they are trying to solve the same problem (regression or classification) by slight different ways. Also, as we have seen in last section, the VAE can be expressed in a Neural network. People have done a lot of research to link them already, another example is the link between Gaussian Naive Bayes classifier and logistic regression. (from <a href="https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">Andrew Ng.</a>).</p>

<p>OK, let’s start step by step. As usual, notations first. Very often, I would ignore the notation part when reading any paper (because it is boring for sure). But please read it this time, not only because it provides us the  characters of the new language, it also helps us to clear some basic concept from high school probability. <a href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library">Here’s a refresher</a> I found very useful for myself.</p>

<blockquote>
  <p><strong>Notations</strong></p>
</blockquote>

<blockquote>
  <ul>
    <li>Uppercase <script type="math/tex">$X</script>$ denotes a <strong>random variable</strong>. Different with deterministic variable, random variable does not have a fixed value, but several possible values with probabilities.</li>
    <li>Uppercase <script type="math/tex">$P(X)</script>$ denotes the probability distribution over that variable. We can say <script type="math/tex">$P(X) \~ N(0,1)</script>$, which means this random variable generates value under a standard normal distribution.</li>
    <li>Lowercase <script type="math/tex">$x \~ P(X)</script>$ denotes a value <script type="math/tex">$x</script>$ sampled from the probability distribution <script type="math/tex">$P(X)</script>$ via some generative process.</li>
    <li>Lowercase <script type="math/tex">$p(X)</script>$ is the density function of the distribution of <script type="math/tex">$X</script>$. It is a scalar function over the measure space <script type="math/tex">$X</script>$.</li>
    <li><script type="math/tex">$p(X=x)</script>$ (shorthand <script type="math/tex">$p(x)</script>$) denotes the density function evaluated at a particular value <script type="math/tex">$x</script>$.</li>
  </ul>
</blockquote>

<p>Now, let’s take a look at the first step. Normally, we are trying to model a dataset from a probability view. For example, we have an image of cat. The pixels in the image is our data (<strong>observation</strong> variable <script type="math/tex">$X</script>$ in probability view). We believe this observable variable is generated from a hidden (latent) variable <script type="math/tex">$Z</script>$, which can be a binary variable (cat or non-cat). We can draw this relationship via the following graph:
<img src="/img/hidden_observation.png" alt="image" /></p>

<table>
  <tbody>
    <tr>
      <td>The edge drawn from <script type="math/tex">$Z</script>$ to <script type="math/tex">$X</script>$ relates the two variables together via the conditional distribution $$$P(X</td>
      <td>Z)<script type="math/tex">$. Now, it's important to jump out of the graph and conditional probability, think about the problem we try to solve, which is given the image, is this an image of cat or not? In the probability language, what's the conditional probability</script>$P(Z</td>
      <td>X)<script type="math/tex">$? Even if we modeled the graph, what we got is the</script>$P(X</td>
      <td>Z)$$$, how can we get to the problem we are interested? <strong>Bayesian</strong> comes to play here.</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">p(Z|X)=\frac{p(X|Z)p(Z)}{p(X)}</script>

<table>
  <tbody>
    <tr>
      <td>Let’s assume we can model the graph $$$p(X</td>
      <td>Z)<script type="math/tex">$ somehow. We can get the final answer if we got</script>$p(Z)<script type="math/tex">$ and</script>$p(X)$$$. In <strong>Bayesian Language</strong>, we have some names for all those math terms. They are just names, but would help you to read paper and discuss with “experts”.</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>Bayesian Language</strong></p>
</blockquote>

<blockquote>
  <ul>
    <li>
      <table>
        <tbody>
          <tr>
            <td>$$$p(Z</td>
            <td>X)$$$ is the <strong>posterior probability</strong>. This is the most important term in Bayesian modeling, because this is the question we are interested.</td>
          </tr>
        </tbody>
      </table>
    </li>
    <li>
      <table>
        <tbody>
          <tr>
            <td>$$$p(X</td>
            <td>Z)<script type="math/tex">$ it the **likelihood**. It means given the hidden variable</script>$Z<script type="math/tex">$, how likely it generates observed images as we have seen in training data. Building this is building the graph. The famous term "maximum likelihood estimation" is one way to solve this. It tries to find the best hidden variable</script>$Z$$$ to lead to good likelihood.</td>
          </tr>
        </tbody>
      </table>
    </li>
    <li><script type="math/tex">$p(Z)</script>$ is the <strong>prior probability</strong>. This captures any prior information we know about <script type="math/tex">$Z</script>$ - for example, if we think that <script type="math/tex">$\frac{1}{3}</script>$ of all images in existence are of cats, then <script type="math/tex">$p(Z=1)=\frac{1}{3}</script>$ and <script type="math/tex">$p(Z=0)=\frac{2}{3}</script>$</li>
    <li><script type="math/tex">$p(X)</script>$ is called <strong>model evidence</strong> or <strong>marginal likelihood</strong>. The way to compute this is marginalizing the likelihood over hidden variable <script type="math/tex">$Z</script>$. 
<script type="math/tex">p(X) = \int{p(X|Z=z)p(Z=z)}dz</script></li>
    <li>Marginalization is the bread and butter of Bayesian modeling, because this gives us the model uncertainty.</li>
  </ul>
</blockquote>

<p>This is the Bayesian language. It’s easy to follow, but too <strong>abstract</strong> to understand, right? Because everything here is probability, but not straight-forward equations we can code up. I’m agree, and feel the same. Now, let’s visualize it through a simple example under <strong>Naive Bayesian Classifier</strong>.</p>

<p><img src="/img/naive_bayes.png" alt="image" /></p>

<table>
  <tbody>
    <tr>
      <td>This is the structure graph of Naive Bayesian classifier. Very similar to the previous graph, but with one assumption, all the observations are conditional independent given the hidden variable. Let’s say we have 3 observed binary variables <script type="math/tex">$X_1, X_2 and X_3</script>$ and one binary hidden variable <script type="math/tex">$X, Z=\\{0,1\\}</script>$. Given a dataset containing the observed variables and hidden variables value <script type="math/tex">% <![CDATA[
$<X, Z> %]]></script>$, how can we learn the graph and how to do the inference (solve the posterior probability) $$$p(Z=1</td>
      <td>X_1=1,X_2=0,X_3=1)$$$?</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>As we have shown above, in order to solve the posterior probability, we need to learn the likelihood $$$p(X</td>
      <td>Z)<script type="math/tex">$, the prior</script>$p(Z)<script type="math/tex">$ and the model evidence</script>$p(X)$$$.</td>
    </tr>
  </tbody>
</table>

<p>It’s easy to get the prior <script type="math/tex">$p(Z=1)</script>$, just estimate it from the training data 
<script type="math/tex">p(Z=1) = \frac{\\#Z==1}{\\#Total}</script></p>

<table>
  <tbody>
    <tr>
      <td>Hard part is the likelihood, $$$p(X_1=x_1,X_2=x_2,X_3=x_3</td>
      <td>Z=1)$$$, which is a conditional joint probability. Thanks to the independency assumption of Naive Bayes, we can write this likelihood like this:</td>
      <td> </td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>$$p(X_1=x_1,X_2=x_2,X_3=x_3</td>
      <td>Z=1) = p(X_1=x_1</td>
      <td>Z=1)p(X_2=x_2</td>
      <td>Z=1)p(X_3=x_3</td>
      <td>Z=1)$$</td>
    </tr>
  </tbody>
</table>

<p>Compute the conditional probability with one variable is easy:</p>

<script type="math/tex; mode=display">% <![CDATA[
p(X_i|Z)=\frac{P(X_i\cap Z)}{P(Z)}=\frac{\\#(X_i \\& Z)}{\\#(Z)} %]]></script>

<p>The model evidence is just a integral of those posteriors.</p>

<p>Now, hope we have a clear picture how does Bayes model works. Keep in mind, the posterior is easy under the Naive Bayes assumptions, but hard (‘nontrackable’) in most cases. You can imagine it would be even harder to compute the model evidence.</p>

<p>Until now, I guess you may have the same question as I have. The “hidden variable” is our target, which is observable in the training data. Many situations, the real “hidden variable” is the variables we do not even know in the training data, such as some edge features in the image. How can we define these kind of problem? How can we present them in the graph?</p>

<p><img src="/img/real_graph.png" alt="image" /></p>

<p>This is a graph defines more complicated and real life problem. Given the training inputs <script type="math/tex">$X=\\{x_1,...,x_N\\}</script>$ and their corresponding outputs <script type="math/tex">$Y=\\{y_1,...,y_N\\}</script>$, in <strong>Bayesian (parametric) modeling</strong>, we would like to find the parameters <script type="math/tex">$\theta</script>$ of a function <script type="math/tex">$y=f^{\theta}(x)</script>$ that are likely to have generated our outputs. In another word, what parameters are likely to have generated out data?</p>

<p>The <strong>model forward (testing/inference)</strong> is not the posterior probability anymore. Given a new input point <script type="math/tex">$x'</script>$ and the training data, we would like to infer what’s the probability of corresponding value of <script type="math/tex">$y</script>$</p>

<script type="math/tex; mode=display">p(y'|x', X, Y) = \int{p(y|x', \theta)p(\theta|X,Y)d\theta}</script>

<p>It can also be written as</p>

<script type="math/tex; mode=display">p(y'|x', X, Y) = \int{f_{\theta}(x')p(\theta|X,Y)d\theta}</script>

<p>We can see that is marginalizing likelihood over posterior. Also remember, in the Bayesian modeling, <script type="math/tex">$\theta</script>$ is not one best value, but a set of possible values with corresponding probabilities. Comparing with the “Bayesian Language” shown above, we need to slightly modify the language definition.</p>

<blockquote>
  <p><strong>Bayesian Language Update</strong></p>
</blockquote>

<blockquote>
  <ul>
    <li>
      <table>
        <tbody>
          <tr>
            <td><strong>Posterior Probability</strong> $$$p(\theta</td>
            <td>X,Y) = \frac{p(Y</td>
            <td>X,\theta)p(\theta)}{p(Y</td>
            <td>X)}$$$</td>
          </tr>
        </tbody>
      </table>
    </li>
    <li>
      <table>
        <tbody>
          <tr>
            <td><strong>Likelihood</strong> $$$p(Y</td>
            <td>X,\theta)$$$</td>
          </tr>
        </tbody>
      </table>
    </li>
    <li><strong>Prior Probability</strong> <script type="math/tex">$p(\theta)</script>$</li>
    <li>
      <table>
        <tbody>
          <tr>
            <td><strong>Model Evidence</strong> $$$p(Y</td>
            <td>X) = \int{p(Y</td>
            <td>X,\theta)p(\theta)d\theta}$$$</td>
          </tr>
        </tbody>
      </table>
    </li>
  </ul>
</blockquote>

<table>
  <tbody>
    <tr>
      <td>The same as previous examples, the most important part is the posterior $$$p(\theta</td>
      <td>X,Y)$$$. It cannot usually be evaluated analytically. Instead we seek some estimations such as MC based sampling method or by an approximating <strong>variational distribution</strong></td>
    </tr>
  </tbody>
</table>

<p>One more point I need to make here is, the output of Bayesian inference is not just a value, but an expectation value and uncertainty. If we deal with the regression problem, the inference can be express as Gaussian likelihood.</p>

<p><script type="math/tex">\mathbf{E}(y') = \int{f_{\theta}(x')p(\theta|X,Y)d\theta}</script> 
<script type="math/tex">var(y') = \tau^{-1} I</script></p>

<p>If we deal with classification problem, the expectation is softmax likelihood. <a href="http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html">Yarin Gal</a> described a way to extract prediction exception in a Bayesian view from Neural Network with dropout, which provides a good link between NN and Bayesian modeling.</p>

<script type="text/javascript">

    // grab all elements in DOM with the class 'equation'
        var tex = document.getElementsByClassName("equation");

            // for each element, render the expression attribute
        Array.prototype.forEach.call(tex, function(el) {
                    katex.render(el.getAttribute("data-expr"), el);
                        
                });

        </script>

