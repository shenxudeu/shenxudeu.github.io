<h2 id="neural-network-infers-bayes">Neural Network Infers Bayes</h2>
<p>Congratulations, you have made to the third and final part! Equipped with the Bayesian language, we can start to look at the “special” regularization term in the VAE loss function and try to make sense of it. Most articles talks about “variational inference” and derive the equations of <strong>variational lower bound</strong> and <strong>KL divergence</strong>. I encourage you to read this <a href="http://blog.evjang.com/2016_08_01_archive.html">blog from Eric Jang</a>for more detail of variational inference. Here, we are going to focus on how to use Neural networks to present Bayesian likelihood and posterior and how to setup the loss function. Let’s start with our old friend, the directed graph of Bayesian modeling.</p>

<p><img src="/img/VAE_graph.png" alt="image" /></p>

<table>
  <tbody>
    <tr>
      <td>Graph (a) is the VAE we are interested in. Since VAE is a unsupervised model, what we want to learn is the hidden random variable <script type="math/tex">$Z</script>$, which is a much lower representation of observed variable <script type="math/tex">$X</script>$. This graph show us the posterior $$$p(Z</td>
      <td>X)<script type="math/tex">$ is **what we want to learn**. Also, please keep in mind, the joint distribution</script>$p(X,Z)<script type="math/tex">$ can be expressed as</script>$p(X</td>
      <td>Z)P(Z)$$$ as shown in this graph. In order to compute the posterior distribution, Bayesian rule comes to convert it into likelihood, prior, and model evidence (the Bayesian language).</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">p(Z|X)=\frac{p(X|Z)p(Z)}{p(X)}</script>

<table>
  <tbody>
    <tr>
      <td>Let’s exam how to compute those terms one by one, starting from likelihood $$$p(X</td>
      <td>Z)<script type="math/tex">$. As we discussed in last section, graph (b) means we can assume there is a function f with parameter</script>$\theta<script type="math/tex">$ to generate the variable</script>$X<script type="math/tex">$, and</script>$X$$$ is follows a Gaussian distribution. Then this likelihood can be expressed as <strong>Gaussian Likelihood</strong> (discussed in last section).</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">p(X|Z) = p(X|Z,\theta) = N(X; f_{\theta}(Z), \tau^{-1}I)</script>

<p>We can further assume <script type="math/tex">$tau^{-1}</script>$ is a diagonal covariance matrix. If we use a neural network to present the function <script type="math/tex">$f_\theta</script>$, this <strong>Gaussian likelihood</strong> can be expressed as this:</p>

<p><img src="/img/gaussian_likelihood_nn.png" alt="image" /></p>

<p>Through the famous <a href="http://blog.evjang.com/2016_08_01_archive.html">reparametrization trick </a>, we can use gradient descent to learn the NN weights <script type="math/tex">$\theta</script>$ with <strong>sampling</strong> given a set of training samples with <script type="math/tex">$Z</script>$ and corresponding <script type="math/tex">$X</script>$</p>

<p>In order to compute the posterior distribution, likelihood is not enough. The hardest term is computing model evidence <script type="math/tex">$p(X)</script>$, which is an integral over all configuration of hidden variables.</p>

<script type="math/tex; mode=display">p(X) = \int{p(X|Z)p(Z)dz}</script>

<p>It requires us to consider all possible of hidden variables, in another word, we need to train tons of neural networks, which is untraceable. Sampling is one way to solve this. It means instead of evaluate all possible hidden variable configurations, we can compute some of them by sampling, but it is still very slow. The way VAE deal with this is called <strong>variational inference</strong>. It says we can try to learn a simple posterior which is easy to compute, and make it similar to the true posterior.</p>

<script type="math/tex; mode=display">q(Z|X,\lambda) = p(Z|X,\theta)</script>

<table>
  <tbody>
    <tr>
      <td>We can assume distribution $$$q(Z</td>
      <td>X,\lambda)$$$ is a multivariate Gaussian. We can visualize this concept by the following graph.</td>
    </tr>
  </tbody>
</table>

<p><img src="/img/q_distr.png" alt="image" /></p>

<p>Assuming the approximation distribution <script type="math/tex">$q</script>$ is Gaussian, we can use similar technique (compute the Gaussian likelihood). The neural network can be like this:</p>

<p><img src="/img/gaussian_posterior_nn.png" alt="image" /></p>

<p>The Gaussian posterior neural network serves as the <strong>encoder</strong> network, and Gaussian likelihood neural network serves as the <strong>decoder</strong> network. Putting them all together, we can get the VAE network shown in section 1.</p>

<p><img src="/img/vae_nn.png" alt="image" /></p>

<p>The last bit is how to define our loss function for this VAE network. As we described above, we need to make the approximation distribution <script type="math/tex">$q</script>$ similar to the true posterior, very straight-forward, <strong>Kullback-Leibler (KL) divergence</strong> is our loss. Let’s take a look at KL divergence definition now:</p>

<script type="math/tex; mode=display">KL(q(Z|X,\lambda)||p(Z|X)) = E_q[log{q(Z|X,\lambda)}] - E_q[log{p(X,Z)} + log{p(X)}]</script>

<table>
  <tbody>
    <tr>
      <td>The beauty of this equation is hard rock posterior $$$p(Z</td>
      <td>X)<script type="math/tex">$ is gone. But wait a minute, the monster term</script>$p(X)<script type="math/tex">$ comes back! Do we still need to deal with this endless integral? The answer is no, we can use some **math trick** to get rid of it. Let me show you the trick. Firstly, we group the first and second term in KL divergence together (times</script>$-1$$$) and call it <strong>Evidence Lower Bound (ELBO)</strong>.</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">ELBO(\lambda) = E_q[log{p(X,Z)} - E_q[log{q(Z|X,\lambda)}]</script>

<p>Then rewrite the KL divergence using <script type="math/tex">$ELBO(\lambda)</script>$</p>

<script type="math/tex; mode=display">KL = log{p(X) - ELBO}</script>

<p>In order to maximize <script type="math/tex">$KL</script>$, we can just minimize <script type="math/tex">$ELBO</script>$ instead, since <script type="math/tex">$KL</script>$ is always positive by definition.</p>

<p>Through some math re-writing, we can get the final loss function as negative of <script type="math/tex">$ELBO</script>$</p>

<script type="math/tex; mode=display">loss = KL(q(Z|X,\theta)||p(Z)) - log{p(X|Z,\lambda)}</script>

<table>
  <tbody>
    <tr>
      <td>$$$log{p(X</td>
      <td>Z,\lambda)}<script type="math/tex">$ is the binary cross-entropy we used as in AutoEncoder, and</script>$KL(q(Z</td>
      <td>X,\theta)</td>
      <td> </td>
      <td>p(Z))$$$ is the regularization term.</td>
    </tr>
  </tbody>
</table>

