<h1 id="variational-autoencoder-vae-is-easy-to-understand">Variational AutoEncoder (VAE) is Easy to Understand</h1>

<p><img src="/img/digits.gif" alt="image" /></p>

<p><img src="/img/faces.png" alt="image" /></p>

<h2 id="before-everything">Before Everything</h2>
<p>I assume you, like me, know a bit of neural networks.</p>

<p>I assume you, also like me, have attempted many times to understand Bayesian and have either failed or reached a state of “almost got it”.</p>

<p>After all, Bayesian people speak a different language from NN people, which can be counterintuitive at times. Among the hardest, there is no Andrej Karpathy yet on this topic. I am not by any measure qualified to be that guy, but rather want to share what I’ve figured out from a maybe practitioner’s point of view.</p>

<p>I plan to spend the time and space of 3 blog posts to make the linkage between NN and Bayesian. The current one is the first: VAE is easy to understand. Following this if you are still interested you would want to check out:</p>

<ul>
  <li><a href="http://shenxudeu.github.io/2016/12/05/bayes-language/">Bayes Language</a></li>
  <li><a href="http://shenxudeu.github.io/2016/12/18/nn-infers-bayes/">NN Infers Bayesian</a></li>
</ul>

<p>OK, let’s get started.</p>

<h2 id="variantial-antoencoder-is-just-a-neural-network">Variantial AntoEncoder is just a Neural Network</h2>
<p>Most articles on VAE (including the original paper) start with posterior distribution estimation, KL divergence, variational inference, etc. Those “fancy” terms means nothing to me at first time. We would get lost easily if trying to understand those terms or trying to follow the equations directly. As an alternative, I prefer to start from a more practical perspective, getting the intuition of this model, understanding how this model works, and playing the code. Then we can ask deeper questions, such as why this model works, what’s the theory behind it.
<img src="/img/vae_as_nn.png" alt="image" /></p>

<p>This is the structure of VAE, which is very similar with a classical <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/">AutoEncoder</a>. We take raw image as network input, going through two simple fully connected layers to project the original dimension (eg. 784 for MINST) into a lower dimension (such as 2). Then use a symmetric 2 fully connected layers to reconstruct it back to the original image dimension. AutoEncoder as well as VAE, can be seen as a data compression model. In the training process, we can just set the training target as input image itself and use reconstruction error as the loss. Choosing an appropriate metric for image reconstruction is hard (but that’s another <a href="https://arxiv.org/abs/1512.09300">story</a>). We’ll use the binary <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sigmoid_cross_entropy_with_logits">cross-entropy</a>, which is commonly used for data like MNIST. Let’s visualize the structure with code (TensorFlow + Keras)</p>

<div class="highlighter-rouge"><pre class="highlight"><code># Classical AutoEncoder
in_x = Input(shape=(784,))
encoded = Dense(128, activation='relu')(in_x)
encoded = Dense(2, activation='relu')(encoded)
decoded = Dense(128, activation='relu')(encoded)
output = Dense(784, activation='sigmoid')(encoded)
</code></pre>
</div>
<p>These 5 lines of code is the classical AutoEncoder, which is just a transfer of the network structure graph. Then, let’s take a look the VAE.</p>

<div class="highlighter-rouge"><pre class="highlight"><code># VAE
in_x = Input(shape=(784,))
encoded = Dense(128, activation='relu')(in_x)

z_mean = Dense(2, activation='relu')(encoded)
z_var = Dense(2, activation='relu')(encoded)
encoded = z_mean + K.exp(z_var / 2) * K.random_normal(shape=tf.shape(z_mean))

decoded = Dense(128, activation='relu')(encoded)
output = Dense(784, activation='sigmoid')(encoded)
</code></pre>
</div>

<p>This is almost the same the AutoEncoder, except we added a random noise on the <code class="highlighter-rouge">z_mean</code>. Intuitively, this random noise serves as a ‘drop-out’ like regularizer.</p>

<p>Another difference between AutoEncoder and VAE is the loss function. Here is the loss of AutoEncoder, which is simply binary cross-entropy.</p>

<div class="highlighter-rouge"><pre class="highlight"><code># Classical AutoEncoder Loss
construction_loss = K.binary_crossentropy(output, in_x)
loss = tf.reduce_mean(construction_loss)
</code></pre>
</div>

<p>This is the VAE loss</p>

<div class="highlighter-rouge"><pre class="highlight"><code># VAE Loss
construction_loss = K.binary_crossentropy(output, in_x)
KL_loss = -0.5 * K.sum(1+ z_var -K.square(z_mean) - K.exp(z_var),axis=-1)

loss = tf.reduce_mean(K.mean(construct_loss, axis=-1) + KL_loss)
</code></pre>
</div>

<p>The only difference is VAE loss has an extra <code class="highlighter-rouge">KL_loss</code>, which is an simple function of the coding variables <code class="highlighter-rouge">z_mean</code> and <code class="highlighter-rouge">z_var</code>. We can also treat this term as an special regularizer. In summary, VAE is just an AutoEncoder with some special regularizer. This regularizer provides some guides what <code class="highlighter-rouge">z_mean</code> and <code class="highlighter-rouge">z_var</code> should look like. And because of this extra regularization term, it performs better than classical AutoEncoder. (We will discuss why the regularizer works in later section.)</p>

<p>Now, let’s have some fun to train this VAE we just built on MNIST. Firstly, let’s train this VAE only with 2 dimension of the coding variables. In this way, we are encoding a 28x28 MNIST image into 2 numbers. The following figure shows the trained 2-D coding variables from training images.</p>

<p><img src="/img/hidden_vars_train.png" alt="image" /></p>

<p>We can see different digits has been separated in the coding space already. Then let’s take a look at how well this model reconstruct our testing (unseen) images.</p>

<p><img src="/img/reconstruct_test.png" alt="image" /></p>

<p>The first row is the true testing image, and second row is reconstructed images. It’s pretty cool, right? Even we can still see some blur in the reconstructed images, it mostly regenerated the raw image. Remember, we compress the 784 dimensions into only <strong>2</strong> dimensions! If we increase the coding dimensions from 2 to 12, here is the reconstructed images. It’s much cleaner!</p>

<p><img src="/img/reconstruct_test_latent12.png" alt="image" /></p>

<p>Also, remember VAE is a generative model, which means we can generate “new” images from nothing! Take the first example, we represent a 28 x 28 image into only <strong>2</strong> numbers. If we just scan the 2-D space and generate digits from there, here is what we can see.</p>

<p><img src="/img/generate_digit_map.png" alt="image" /></p>

<p>We can generate any digit from 0 to 9. If we start from (-0.15,-0.15) and slowly move those numbers to (0.15, 0.15), we can see the beautiful figure in the beginning.</p>

